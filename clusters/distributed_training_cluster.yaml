cluster_name: workflow-train-distributed

provider:
    type: aws
    region: us-east-1
    cache_stopped_nodes: False

auth:
    ssh_user: ubuntu

available_node_types:
  ray.head.default:
      node_config:
        # https://aws.amazon.com/ec2/instance-types/g4/
        InstanceType: g4dn.12xlarge  #  p3dn.24xlarge; g3.16xlarge; p3.2xlarge
#        InstanceMarketOptions:
#            MarketType: spot
        BlockDeviceMappings:
            - DeviceName: /dev/sda1
              Ebs:
                  VolumeSize: 200
        ImageId: ami-0d83f8559c6480f2f  # exoflow-artifacts-0419
        SecurityGroupIds:
          - "sg-3463e565"
          - "sg-092b10044bcf1f37e"
  ray.worker.default:
      node_config:
        InstanceType: r3.2xlarge  # high-mem!
        ImageId: ami-0d83f8559c6480f2f  # exoflow-artifacts-0419
        SecurityGroupIds:
          - "sg-3463e565"
          - "sg-092b10044bcf1f37e"
      min_workers: 1
      max_workers: 1


# setup_commands:
#     # This replaces the standard anaconda Ray installation
#     - mkdir -p ~/efs
#     - sudo mount -t efs fs-d416cc55:/ ~/efs
#     - sudo chmod 777 ~/efs

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - "sudo ulimit -n 65536; PYTHONPATH=/exoflow/experiments/distributed_training ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --resources='{\"machine\": 1, \"tag:gpu\": 1}' --storage=s3://exoflow"

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - "sudo ulimit -n 65536; PYTHONPATH=/exoflow/experiments/distributed_training ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --resources='{\"machine\": 1, \"tag:data\": 1}' --storage=s3://exoflow"
